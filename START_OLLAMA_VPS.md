# Start Ollama Service on VPS

## Issue

Ollama service exists in `docker-compose.yml` but isn't running. Fix:

## Steps

### 1. Start Ollama Service

```bash
cd /opt/progrc/bff-service-backend-dev
docker-compose up -d ollama
```

### 2. Wait for Ollama to Start

```bash
docker-compose ps ollama
# Should show "Up" status
```

### 3. Pull Required Models

```bash
# LLM model (already pulled)
docker-compose exec ollama ollama pull llama3.2:1b

# Embedding model (required for chunk analysis)
docker-compose exec ollama ollama pull nomic-embed-text
```

### 4. Fix .env Configuration

```bash
nano .env
```

Change:
```
OLLAMA_BASE_URL=http://ollama:11434
```

**NOT** `http://168.231.70.205:11434` (use Docker service name)

### 5. Restart Backend

```bash
docker-compose restart app
```

### 6. Verify

```bash
# Check Ollama is running
docker-compose ps ollama

# Check models
docker-compose exec ollama ollama list

# Check backend using Ollama
docker-compose logs app | grep -i "Using Ollama"
```

Should see: "Using Ollama for AI processing (local, no API calls)"

## Why This Matters

- **Source Processing**: Needs Ollama to analyze chunks against controls
- **Percentage Scoring**: Uses chunk relevance scores from Ollama analysis
- **Control Statements**: Generated by Ollama LLM processing
